# Fraud Anomaly Detection using Unsupervised Learning

This project implements an end-to-end **credit card fraud detection system**
using **unsupervised anomaly detection techniques**. The objective is to detect
fraudulent transactions in a highly imbalanced dataset without relying on fraud
labels during training.

Multiple anomaly detection models are evaluated, and a final model is selected
based on detection performance, robustness, and interpretability.

---

## Project Highlights

- End-to-end ML pipeline (EDA ‚Üí preprocessing ‚Üí modeling ‚Üí comparison)
- Unsupervised anomaly detection (no fraud labels used during training)
- Threshold tuning to balance precision and recall
- Model interpretability using reconstruction error
- API-ready design for real-time inference
- Planned deployment using **Hugging Face Spaces**

## Models Implemented

The following unsupervised anomaly detection models are implemented and evaluated:

- **Isolation Forest**
- **One-Class SVM**
- **Autoencoder (final selected model)**

The **Autoencoder** is selected as the final model due to:
- Best balance between precision and recall
- Highest F1-score after threshold tuning
- Lower false-positive rate compared to other models
- Feature-level interpretability via reconstruction errors
- Better suitability for real-time API deployment

---

## Final Model Performance (Autoencoder)

Threshold selected using sensitivity analysis: **99.3 percentile**

| Metric | Value |
|------|------|
| Precision | 0.72 |
| Recall | 0.59 |
| F1-score | 0.65 |
| ROC-AUC | 0.94 |

This configuration achieves a strong balance between detecting fraudulent
transactions and maintaining a manageable false-positive rate.

---

## Interpretability

Interpretability is a key requirement in fraud detection systems.

The autoencoder provides interpretability through **reconstruction error analysis**:
- Each transaction is reconstructed by the model
- Feature-wise reconstruction errors are computed
- Features with the highest reconstruction error indicate abnormal behavior

This enables the system to explain *why* a transaction is flagged as fraudulent,
rather than acting as a black box.

---

## ‚öôÔ∏è Setup Instructions

### 1. Clone the Repository


git clone https://github.com/<your-username>/Fraud_Anomaly_Detection.git
cd Fraud_Anomaly_Detection

### 2. Create a Virtual Environment
python -m venv .venv


Activate the environment:

Windows

.venv\Scripts\activate


Linux / macOS

source .venv/bin/activate

### 3. Install Dependencies
pip install -r requirements.txt

üì• Dataset Instructions

This project uses the Credit Card Fraud Dataset.

Steps:

Download the dataset from Kaggle

Place the file at:

data/raw/creditcard.csv


The dataset is not included in the repository due to size constraints.

‚ñ∂Ô∏è Execution Order (IMPORTANT)

Run the notebooks strictly in the following order:

01_data_loading_and_enrichment.ipynb
02_eda.ipynb
03_preprocessing.ipynb
04_isolation_forest.ipynb
05_one_class_svm.ipynb
06_autoencoder.ipynb
07_model_comparison.ipynb


Each notebook depends on outputs generated by previous steps.

üåê API Design

The project includes a REST API implemented using FastAPI.

API Capabilities:

Accepts transaction features as input

Returns:

Fraud probability score

Binary fraud decision

Explanation based on top anomalous features

Example API Response:
{
  "fraud_probability": 0.82,
  "is_fraud": true,
  "top_contributing_features": [
    "Amount",
    "distance_from_home",
    "hour"
  ]
}

 ### Deployment

The API is designed to be deployed on Hugging Face Spaces using FastAPI.

Deployment benefits:

Free and reproducible hosting

Public demo link

Easy evaluation and sharing

No infrastructure management required

Deployment instructions and live demo link will be added once the API
implementation is complete.

üìù Notes

Model artifacts (.npy, .pkl) are generated locally and excluded from Git

Threshold tuning is performed without retraining models

Fraud labels are used only for evaluation, not training

TensorFlow autoencoder experiments were conducted separately for comparison

The scikit-learn autoencoder is selected for final deployment

### Author

Raj Thumar
M.Tech Data Science
Indian Institute of Technology Guwahati


---



